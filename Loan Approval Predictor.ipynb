# **Project: Loan Approval Predictor**

# %% [markdown]
# Goal:
# Predict if a person’s loan will be approved based on features like gender, marital status, income, credit history, etc.

# %%
# STEP 1: Loading libraries

import pandas as pd

# %%
#Loading dataset
 
df = pd.read_csv(r'C:\Users\aayus.AAYUSH_LOQ\Documents\AiMl\DATASET\loan.csv')

# %%
df.head()   #checking the first 5 rows of the dataset

# %%
import matplotlib.pyplot as plt

importance = abs(model.coef_[0])
features = X.columns

plt.barh(features, importance)
plt.title("Feature Importance")
plt.show()


# %%
# STEP 2: Checking missing values

df.info()

# %%
df.isnull().sum()          #checking the number of missing values in each column

# %%
# STEP 3: Dropping unnecessary columns

df.drop('Loan_ID', axis=1, inplace=True) 

#here axis=1 for rows and axis=0 for columns. If True then main CSV modifies and if False then new CSV is created which we needed to assign again.

# %% [markdown]
# We'll fill missing values based on data type:
# 
# -Text (object): use most frequent value (mode)
# 
# -Numbers: use median or mean

# %%
# STEP 4: Fill missing values

# Text: use most frequent value (mode), Numbers: use median or mean

# Fill categorical (text) columns with mode
for col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Credit_History']:
    df[col] = df[col].fillna(df[col].mode()[0])  #mode returns series of value so we use [0] to get 1 number

# Fill numerical columns with median
for col in ['LoanAmount', 'Loan_Amount_Term']:
    df[col] = df[col].fillna(df[col].median())  #median returns single value so no need to use [0]


# %% [markdown]
# ML models only understand numbers. So we’ll convert:
# 
# -Binary text → Label Encoding
# 
# -Multi-category text → One-Hot Encoding
# 

# %%
# STEP 5: Encode Text

from sklearn.preprocessing import LabelEncoder

# List of binary columns
binary_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Loan_Status']
# Label encoding
le = LabelEncoder()
for col in binary_cols:
    if col in df.columns:
        df[col] = le.fit_transform(df[col].astype(str))  # Use str in case of any leftover NaNs

#  One-Hot Encode
one_hot_cols = [col for col in ['Property_Area', 'Dependents'] if col in df.columns]

df = pd.get_dummies(df, columns=one_hot_cols, drop_first=True)

# %% [markdown]
# We'll scale income and loan amount using Standard Scaler (brings values to mean=0 and std=1).

# %%
# STEP 6: Feature Scaling


from sklearn.preprocessing import StandardScaler

# Step 1: Columns we want to scale
cols_to_scale = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']

# Step 2: Find which ones still exist in df
existing_cols = [col for col in cols_to_scale if col in df.columns]

# Step 3: Only apply scaling if columns are found
if existing_cols:
    scaler = StandardScaler()
    df[existing_cols] = scaler.fit_transform(df[existing_cols])
    print("✅ Scaling applied to:", existing_cols)
else:
    print("⚠️ None of the scaling columns found in the DataFrame.")



# %% [markdown]
# We'll split data into:
# 
# X = features (input)
# 
# y = target column (Loan_Status)
# Then split into 80% training and 20% testing.

# %%
from sklearn.model_selection import train_test_split

X = df.drop('Loan_Status', axis=1)
y = df['Loan_Status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# %%
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)


# %%
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))


# %%
import joblib
joblib.dump(model, 'loan_model.pkl')


# %% [markdown]
# 
